In the first week, I learned the basics of clustering and how to use it. I examined algorithms and took some notes. I started by learning about what unsupervised clustering is for. Its main goal is to group similar data points based on distance or similarity metrics. After that, I looked at some important algorithms, such as K-Means, K-Median, and K-Center. K-Means tries to find the smallest squared distances, K-Median tries to find the smallest L1 distances to be less affected by outliers, and K-Center tries to find the smallest worst case distance.

In addition to the main clustering methods, I looked into a number of other important topics that make clustering systems easier to understand, fairer, and more scalable. We talked about Principal Component Analysis (PCA) as a way to cut down on the number of dimensions in a dataset. This can make clustering work better, especially with data that is noisy or has a lot of dimensions. I also learned about "coresets," which are small, weighted groups of data that are almost the same as full clustering results. These are important for getting algorithms to work with large or streaming datasets. These methods are helpful for tasks that involve large spatial or temporal datasets because they can cut down on the amount of work that needs to be done while still making sure that the clustering is good.

Finally, I looked into how to change clustering to make it work better in the real world, like making sure that demographics are fair, making it easier to understand, and picking the best parameters. It's important to use fair clustering methods when making systems like public service distribution. 

This week was mostly about learning and going over the basics again, building a strong conceptual and theoretical base before moving on to implementation.